- name: set hdfs fact
  set_fact: hdfs_installed=True

- name: Create service account for HDFS
  user: name={{ hadoop.user }}
        system=yes
        shell={{ hadoop.user_shell }}
        state=present
        groups="{{ hadoop.user_groups | join(',') }}"

- name: allow access to all hosts
  include_role:
    name: purpose/pwdlessssh
  vars:
    pwdless: "{{ hadoop.user }}"

- name: install java
  apt:
    name: openjdk-11-jdk

- name: Set JAVA_HOME
  lineinfile:
    dest: /etc/environment
    state: present
    regexp: '^JAVA_HOME'
    line: 'JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64'

- name: define number of nodes
  set_fact: number_of_nodes="{{ groups['kvm_vm'] | length | int }}"

- debug:
    msg: "Number of data nodes: {{ number_of_nodes }}"

- name: disable hdfs replication in single node
  set_fact: hdfs_replication_factor=1
  when: number_of_nodes | int < 3

- name: enable hdfs replication when cluster has more then three nodes
  set_fact: hdfs_replication_factor=3
  when: number_of_nodes | int >= 3

- debug:
    msg: "HDFS replication factor: {{ hdfs_replication_factor }}"

- name: remove pre-existent hadoop data directory
  file:
    path: "{{ hadoop.data_dir }}/"
    state: absent

- name: create hadoop data directory
  file:
    path: "{{ hadoop.data_dir }}"
    state: directory
    owner: "{{ hadoop.user }}"
    group: "{{ hadoop.user }}"
  become: true

- name: remove pre-existent hadoop name directory
  file:
    path: "{{ hadoop.name_dir }}/"
    state: absent

- name: create hadoop name directory
  file:
    path: "{{ hadoop.name_dir }}"
    state: directory
    owner: "{{ hadoop.user }}"
    group: "{{ hadoop.user }}"
  become: true


- name: remove pre-existent hadoop temp directory
  file:
    path: "{{ hadoop.temp_dir }}/"
    state: absent

- name: create hadoop temp directory
  file:
    path: "{{ hadoop.temp_dir }}"
    state: directory
    owner: "{{ hadoop.user }}"
    group: "{{ hadoop.user }}"
  become: true

- name: create install directory
  file:
    path: "/opt/{{ hadoop.hadoop_install_dir }}"
    state: directory
    owner: "{{ hadoop.user }}"
    group: "{{ hadoop.user }}"
  become: true

- debug:
    msg: "Downloading Hadoop from: {{ hadoop.download_location }}/hadoop-{{ hadoop.version }}/{{ hadoop.hadoop_archive }}"

- name: download hadoop
  get_url: url="{{ hadoop.download_location }}/hadoop-{{ hadoop.version }}/{{ hadoop.hadoop_archive }}" dest="/tmp/{{ hadoop.hadoop_archive }}"

- name: unarchive to the install directory
  shell: "tar -xvf /tmp/{{ hadoop.hadoop_archive }} --strip 1 --directory /opt/{{ hadoop.hadoop_install_dir }}"

- name: set core-site.xml
  template: src="core-site.xml.j2" dest="/opt/{{ hadoop.hadoop_install_dir }}/etc/hadoop/core-site.xml"

- name: set hadoop-env.sh
  template: src="hadoop-env.sh.j2" dest="/opt/{{ hadoop.hadoop_install_dir }}/etc/hadoop/hadoop-env.sh"

- name: set hdfs-site.xml
  template: src="hdfs-site.xml.j2" dest="/opt/{{ hadoop.hadoop_install_dir }}/etc/hadoop/hdfs-site.xml"

- name: set slaves
  template: src="slaves.j2" dest="/opt/{{ hadoop.hadoop_install_dir }}/etc/hadoop/slaves"
  become: true

# Environment setup.
- name: add hadoop profile to startup
  template:
    src: hadoop-profile.sh.j2
    dest: /etc/profile.d/hadoop-profile.sh
    mode: 0644

- name: format hdfs
  shell: "bin/hdfs namenode -format"
  args:
    chdir: "/opt/{{ hadoop.hadoop_install_dir }}"
  when: (inventory_hostname in groups['docker_swarm_manager'])
